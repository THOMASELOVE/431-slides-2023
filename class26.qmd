---
title: "431 Class 26"
author: Thomas E. Love, Ph.D.
date: "2023-12-07"
format:
  revealjs: 
    theme: default
    self-contained: true
    slide-number: true
    footnotes-hover: true
    preview-links: auto
    date-format: iso
    logo: 431-2023-pic.png
    footer: "431 Class 26 | 2023-12-07 | <https://thomaselove.github.io/431-2023/>"
---

## Today's Agenda

1. Graphical and Numerical Summaries of Data
2. 432 preview: A `tidymodels` example with interaction
3. Takeaways from 431

## Today's Packages

```{r}
#| message: false
#| warning: false
library(datasauRus)
```

```{r}
#| echo: true
#| message: false

library(knitr); library(kableExtra)
library(janitor); library(mosaic)
library(patchwork); library(broom)
library(tidyverse)

theme_set(theme_bw())
```

and a couple of secrets, hidden for now.

# Visualizing Data

## New Data Set 1

```{r}
df <- datasaurus_dozen

df <- df |>
  mutate(set = as.numeric(factor(dataset)))

temp <- df |> filter(set == 1)

df_stats(~ y + x, data = temp) |>
  select(response, n, missing, mean, sd) |>
  kable(dig = 2)

temp <- df |> filter(set == 1) 
ggplot(temp, aes(x = x, y = y)) +
  geom_point() +
  labs(title = "Data Set 1",
       subtitle = "Pearson correlation = -0.06") +
  theme(aspect.ratio = 1)
```


## 13 Data Sets in the `df` tibble:

```{r}
df |> group_by(set) |>
  summarize(n = n(), mean_x = mean(x), 
            sd_x = sd(x), 
            mean_y = mean(y), 
            sd_y = sd(y), 
            corr_xy = cor(x, y)) |>
   round_half_up(digits = 2)
```

## New Data: Model for Set 1

```{r}
#| echo: true

set_1 <- lm(y ~ x, data = df |> filter(set == 1))

tidy(set_1, conf.int = T, conf.level = 0.9) |> 
  select(-statistic, -p.value) |> kable(digits = 2)

glance(set_1) |>
  select(r.squared, adj.r.squared, sigma, BIC, p.value) |>
  kable(digits = 3)
```

## All 13 Models, at a glance {.smaller}

```{r}
set_1 <- lm(y ~ x, data = df |> filter(set == 1))
set_2 <- lm(y ~ x, data = df |> filter(set == 2))
set_3 <- lm(y ~ x, data = df |> filter(set == 3))
set_4 <- lm(y ~ x, data = df |> filter(set == 4))
set_5 <- lm(y ~ x, data = df |> filter(set == 5))
set_6 <- lm(y ~ x, data = df |> filter(set == 6))
set_7 <- lm(y ~ x, data = df |> filter(set == 7))
set_8 <- lm(y ~ x, data = df |> filter(set == 8))
set_9 <- lm(y ~ x, data = df |> filter(set == 9))
set_10 <- lm(y ~ x, data = df |> filter(set == 10))
set_11 <- lm(y ~ x, data = df |> filter(set == 11))
set_12 <- lm(y ~ x, data = df |> filter(set == 12))
set_13 <- lm(y ~ x, data = df |> filter(set == 13))

a1 <- glance(set_1) |> mutate(dataset = 1)
a2 <- glance(set_2) |> mutate(dataset = 2)
a3 <- glance(set_3) |> mutate(dataset = 3)
a4 <- glance(set_4) |> mutate(dataset = 4)
a5 <- glance(set_5) |> mutate(dataset = 5)
a6 <- glance(set_6) |> mutate(dataset = 6)
a7 <- glance(set_7) |> mutate(dataset = 7)
a8 <- glance(set_8) |> mutate(dataset = 8)
a9 <- glance(set_9) |> mutate(dataset = 9)
a10 <- glance(set_10) |> mutate(dataset = 10)
a11 <- glance(set_11) |> mutate(dataset = 11)
a12 <- glance(set_12) |> mutate(dataset = 12)
a13 <- glance(set_13) |> mutate(dataset = 13)

a <- bind_rows(a1, a2, a3, a4, a5, a6, a7, a8, 
               a9, a10, a11, a12, a13) |>
  select(dataset, r.squared, adj.r.squared, sigma, 
         AIC, BIC) 

a |> kable(digits = c(0, 3, 3, 0, 0, 0)) |> kable_styling(font_size = 24)
```

## Plot for Data Set 1

Does a linear model for y using x seem appropriate?

```{r}
temp <- df |> filter(set == 1)

ggplot(temp, aes(x = x, y = y)) +
  geom_point() +
  labs(title = "Data Set 1") +
  theme(aspect.ratio = 1)
```

## https://xkcd.com/1725/

![](c26/figures/linear_regression.png)

## Set 1 Plot (+lm, +loess)

```{r, echo = FALSE}
temp <- df |> filter(set == 1)

p1 <- ggplot(temp, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE, col = "red") +
    labs(title = "Data Set 1 with lm fit") +
    theme(aspect.ratio = 1)

p2 <- ggplot(temp, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = "loess", formula = y ~ x, se = FALSE) +
    labs(title = "Data Set 1 with default loess smooth") +
    theme(aspect.ratio = 1)

p1 + p2 
```

### Model 1 (linear model for Set 1)

```{r}
set_1
```


## Residual Plots for Set 1 Model

```{r}
#| fig-height: 6
par(mfrow=c(2,2)); plot(set_1)
```

## The Other 12 Data Sets

Models 2-13 all look about the same in terms of means, medians, correlations, regression models, but what happens if we plot the data?

```{r}
#| echo: true
#| output-location: slide

temp2 <- df |> filter(set != 1) 

ggplot(temp2, aes(x = x, y = y, color = dataset)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~ set, labeller = "label_both")
```

## Actually, each set has a name

```{r}
ggplot(temp2, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ dataset, labeller = "label_both")
```

## And a linear model yields the same fit for each

```{r}
ggplot(temp2, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", col = "red", formula = y ~ x) +
  facet_wrap(~ dataset, labeller = "label_both")
```

## How about a loess smooth with default `span`?

```{r}
ggplot(temp2, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "loess", col = "blue", se = FALSE, formula = y ~ x) +
  facet_wrap(~ dataset, labeller = "label_both", 
             scales = "free_y")
```


## And the data come from

These are, of course, the datasauRus dozen data sets described in Spiegelhalter, available in the `datasauRus` package, which you can install from CRAN, thanks to the work of Steph Locke.

```
library(datasauRus)
df <- datasaurus_dozen
```

- These were created by Alberto Cairo, who has some great books like *How Charts Lie*

The moral of the story: **Never trust summary statistics alone, always visualize your data**

## Two Cool Things, available online...

1. We'll visit Tomas Westlake's work at <https://r-mageddon.netlify.app/post/reanimating-the-datasaurus/>

```{r}
#| echo: true
#| eval: false
library(datasauRus)
library(ggplot2)
library(gganimate)

ggplot(datasaurus_dozen, aes(x=x, y=y))+
  geom_point()+
  theme_minimal() +
  transition_states(dataset, 3, 1)
```


## Two Cool Things, available online...

2. Next, we'll visit <https://www.autodesk.com/research/publications/same-stats-different-graphs>

This is Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice.

We'll look at a couple of the animated plots they generate there.

---

![](c26/figures/elephant.png)


# A Taste of 432: Sea Urchins and Tidy Modeling

---

![](c26/figures/urchins.png)

## Sea Urchins and Tidy Modeling {.smaller}

Constable (1993) compared the inter-radial suture widths of urchins maintained on one of three food regimes 

- Initial: no additional food supplied above what was in the initial sample
- Low: food supplied periodically 
- High: food supplied ad libitum (as often as desired)

In an attempt to control for substantial variability in urchin sizes, the initial body volume of each urchin was measured as a covariate.

- This example comes from https://www.tidymodels.org/start/models/
- Another key source is https://www.flutterbys.com.au/stats/tut/tut7.5a.html
- Data from Constable, A.J. The role of sutures in shrinking of the test in Heliocidaris erythrogramma (Echinoidea: Echinometridae). *Marine Biology* 117, 423-430 (1993). https://doi.org/10.1007/BF00349318

## Package Load / Data ingest (Sea Urchins)

```{r}
#| echo: true
#| message: false
#| warning: false

library(tidymodels)
library(readr)
library(broom.mixed)

urchins <-
  # Data were assembled for a tutorial 
  # at https://www.flutterbys.com.au/stats/tut/tut7.5a.html
  read_csv("https://tidymodels.org/start/models/urchins.csv") |> 
  # Change the names to be a little more verbose
  setNames(c("food_regime", "initial_volume", "width")) |> 
  mutate(food_regime = 
           factor(food_regime, 
                  levels = c("Initial", "Low", "High")))
```

## The `urchins` data

For each of 72 sea urchins, we know their

- experimental feeding regime group (`food_regime`: either Initial, Low, or High),
- size in milliliters at the start of the experiment (`initial_volume`), and
- suture width at the end of the experiment (`width`).

```{r}
#| echo: true
glimpse(urchins)
```

## Plot the Data

```{r}
ggplot(urchins,
       aes(x = initial_volume, y = width, 
           group = food_regime, col = food_regime)) + 
  geom_point() + geom_smooth(method = lm, formula = y ~ x, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)
```

## How should we model the data? {.smaller}

Since the slopes appear to be different for at least two of the feeding regimes, let's build a model that allows for two-way interactions. We'll use a linear model for width which allows each food regime to generate a different slope and intercept for the effect of initial volume.

```{r}
#| echo: true
lm(width ~ initial_volume * food_regime, data = urchins) |> tidy() |> 
  select(term, estimate) |> kable(dig = 4) |> kable_styling(font_size = 24)
```

## Setting up a linear regression with `tidymodels`

```{r}
#| echo: true
lm_mod <-
  linear_reg()  |> 
  set_engine("lm")

lm_mod
```

It turns out that we'll have several options for engines here.

## We can estimate or train the model with `fit()`

```{r}
#| echo: true
lm_fit <- 
  lm_mod |>
  fit(width ~ initial_volume * food_regime, data = urchins)
```

We'll look at the results on the next slide.

## What's in `lm_fit`?

```{r}
#| echo: true
tidy(lm_fit, conf.int = TRUE) |> select(term, estimate, conf.low, conf.high) |> 
  kable(dig = 4) |> kable_styling(font_size = 28)
```

## Make Predictions

Suppose that, for a publication, it would be particularly interesting to make a plot of the mean body size for urchins that started the experiment with an initial volume of 20ml. To create such a graph, we start with some new example data that we will make predictions for.

```{r}
#| echo: true
new_points <- expand.grid(initial_volume = 20, 
                          food_regime = c("Initial", "Low", "High"))

new_points
```

## Obtain Predicted Results for these `new_points`

We'll develop mean predictions and uncertainty intervals.

```{r}
#| echo: true
mean_pred <- predict(lm_fit, new_data = new_points)
conf_int_pred <- predict(lm_fit, 
                         new_data = new_points, 
                         type = "conf_int")
plot_data <- 
  new_points |> 
  bind_cols(mean_pred) |> 
  bind_cols(conf_int_pred)
```

## Plot the `plot_data` results

```{r}
#| echo: true
#| output-location: slide

ggplot(plot_data, aes(x = food_regime, y = .pred)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "urchin size",
       title = "Linear model fit using `lm`")
```

## Could we use Bayesian methods?

Would the results be different if we used a Bayesian approach?

- Need to select a prior.
- Let's use bell-shaped priors on the intercepts and slopes, using a Cauchy distribution (works out to be the same as a t distribution with one degree of freedom)
- The `stan_glm()` function can be used, and this is available as an engine in `tidymodels`, where we need to specify `prior` and `prior_intercept` to fit a linear model.

## Setting up a Bayesian Model

```{r}
#| echo: true
prior_dist <- rstanarm::student_t(df = 1)

set.seed(123)

bayes_mod <-   
  linear_reg() |> 
  set_engine("stan", 
             prior_intercept = prior_dist, 
             prior = prior_dist) 
```

## Training the Bayes Model

```{r}
#| echo: true

bayes_fit <- bayes_mod |> 
  fit(width ~ initial_volume * food_regime, data = urchins)

tidy(bayes_fit, conf.int = TRUE) |> 
  select(term, estimate, conf.low, conf.high) |> 
  kable(dig = 4) |> kable_styling(font_size = 24)
```

## Building the plot for the Bayes model 

```{r}
#| echo: true
#| output-location: slide

bayes_plot_data <- 
  new_points |> 
  bind_cols(predict(bayes_fit, new_data = new_points)) |> 
  bind_cols(predict(bayes_fit, new_data = new_points, 
                    type = "conf_int"))

ggplot(bayes_plot_data, aes(x = food_regime, y = .pred)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), 
                width = .2) + 
  labs(y = "urchin size",
       title = "Bayesian model with t(1) prior distribution")
```


## Comparing the Models

```{r}
p1 <- ggplot(plot_data, aes(x = food_regime, y = .pred)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  geom_label(aes(label = round_half_up(.pred,3)), fill = "white") +
  labs(y = "urchin size",
       title = "Linear model fit using `lm`")

p2 <- ggplot(bayes_plot_data, aes(x = food_regime, y = .pred)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) + 
  geom_label(aes(label = round_half_up(.pred,3)), fill = "white") +
  labs(y = "urchin size",
       title = "Bayesian model with t(1) prior")

p1 + p2 + 
  plot_annotation(title = "Comparing linear models for urchins data")
```

## The models aren't actually the same

```{r}
p1 <- ggplot(plot_data, aes(x = food_regime, y = .pred)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  geom_label(aes(label = round_half_up(.pred,5)), fill = "white") +
  labs(y = "urchin size",
       title = "Linear model fit using `lm`")

p2 <- ggplot(bayes_plot_data, aes(x = food_regime, y = .pred)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) + 
  geom_label(aes(label = round_half_up(.pred,5)), fill = "white") +
  labs(y = "urchin size",
       title = "Bayesian model with t(1) prior")

p1 + p2 + 
  plot_annotation(title = "Comparing linear models for urchins data")
```

## What are we plotting, actually?

```{r}
#| echo: true

plot_data |> kable(dig = 4) |> kable_styling(font_size = 28)
bayes_plot_data |> kable(dig = 4) |> kable_styling(font_size = 28)
```

# What do we take away from 431 at the end of the day?

## Ten Simple Rules for Effective Statistical Practice

From [PLoS Computational Biology](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004961)

1. Statistical Methods Should Enable Data to Answer Scientific Questions
2. Signals Always Come with Noise
3. Plan Ahead, Really Ahead
4. Worry About Data Quality
5. Statistical Analysis Is More Than a Set of Computations

## Ten Simple Rules for Effective Statistical Practice

From [PLoS Computational Biology](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004961)

6. Keep it Simple
7. Provide Assessments of Variability
8. Check Your Assumptions
9. When Possible, Replicate!
10. Make Your Analysis Reproducible

## The Impact of Study Design (Gelman) {.smaller}

Applied statistics is hard. 

- Doing a statistical analysis is like playing basketball, or knitting a sweater. You can get better with practice.
- Incompetent statistics does not necessarily doom a research paper: some findings are solid enough that they show up even when there are mistakes in the data collection and data analyses. But we've also seen many examples where incompetent statistics led to conclusions that made no sense but still received publication and publicity.
- We should be thinking not just about data analysis, but also data quality.

> To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of. (R. A. Fisher)

---

![](c26/figures/bear.jpg)


## What does sad p-value bear lead to? {.smaller}

So you collected data and analyzed the results. Now you want to do an after data gathering (post hoc) power analysis.

1. What will you use as your "true" effect size? 
    - Often, point estimate from data - yuck - results very misleading - power is generally seriously overestimated when computed on the basis of statistically significant results.
    - Much better (but rarer) to identify plausible effect sizes based on external information rather than on your sparkling new result.
2. What are you trying to do? (too often)
    - get researcher off the hook (I didn't get p < 0.05 because I had low power - an alibi to explain away non-significant findings) or
    - encourage overconfidence in the finding.
    
None of this is particularly smart.

## Build Tidy Data Sets {.smaller}

- Each variable you measure should be in one column.
- Each different observation of that variable should be in a different row.
- There should be one table for each "kind" of variable.
- If you have multiple tables, they should include a column in the table that allows them to be linked.
- Include a row at the top of each data table that contains real row names. `Age_at_Diagnosis` is a much much better name than `ADx`.
- Build useful codebooks.

Jeff Leek: "[How to share data with a statistician](https://github.com/jtleek/datasharing)"



## A Tip from David Robinson

![](c26/figures/gradschool.png)


## Ten of the Most Important 431 Ideas

1. You have to visualize and count data to understand it.
2. 90% of statistical work could be described as data management.
3. R Markdown and the tidyverse make it easier to do the right thing.
4. Statistical significance is not a helpful concept.
5. Point estimates and confidence intervals are useful ideas.

## Ten of the Most Important 431 Ideas

6. Most statistical procedures are in fact regression models.
7. All statistical methods involve assumptions worth checking.
8. The bootstrap is a very useful, and somewhat underused tool.
9. Prediction models need to predict well in new situations.
10. Statistical thinking is far too important to be left to statisticians.


---

![](c26/figures/allfolks.png)


## Session Information

```{r}
#| echo: true
sessionInfo()
```

